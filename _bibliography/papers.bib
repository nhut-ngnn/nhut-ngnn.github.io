---
---
@article{nguyen2026HemoGAT,
  abbr={AEEE},
  bibtex_show=True,
  title = {HemoGAT: Heterogeneous Multimodal Speech Emotion Recognition with Cross-Modal Transformer and Graph Attention Network},
  author = {Nguyen, Nhut Minh and Nguyen, Thanh Trung and Dang, Duc Ngoc Minh},
  journal={Advances in Electrical and Electronic Engineering},
  year = {2026},
  code = {https://github.com/nhut-ngnn/HemoGAT},
  note = {To appear},
  preview = {HemoGAT.jpg}
}

@INPROCEEDINGS{nguyen2025CemoBAM,
  abbr={APNOMS},
  bibtex_show=True,
  title = {CemoBAM: Advancing Multimodal Emotion Recognition through Heterogeneous Graph Networks and Cross-Modal Attention Mechanisms},
  author = {Nguyen, Nhut Minh and Le, Thu Thuy and Nguyen, Thanh Trung and Phan, Duc Tai and Tran, Anh Khoa and Dang, Duc Ngoc Minh},
  booktitle = {2025 Asia-Pacific Network Operations and Management Symposium (APNOMS)},
  doi = {10.23919/apnoms67058.2025.11181320},
  year = {2025},
  month = {September},
  abstract ={Multimodal Speech Emotion Recognition (SER) offers significant advantages over unimodal approaches by integrating diverse information streams such as audio and text. However, effectively fusing these heterogeneous modalities remains a significant challenge. We propose CemoBAM, a novel dualstream architecture that effectively integrates the Heterogeneous Graph Attention Network (CH-GAT) with the Cross-modal Convolutional Block Attention Mechanism (xCBAM). In CemoBAM architecture, the CH-GAT constructs a heterogeneous graph that models intra- and inter-modal relationships, employing multi-head attention to capture fine-grained dependencies across audio and text feature embeddings. The xCBAM enhances feature refinement through a cross-modal transformer with a modified 1D-CBAM, employing bidirectional cross-attention and channel-spatial attention to emphasize emotionally salient features. The CemoBAM architecture surpasses previous state-of-the-art (SOTA) methods by 0.32% on IEMOCAP and 3.25% on ESD datasets. Comprehensive ablation studies validate the impact of Top-K graph construction parameters, fusion strategies, and the complementary contributions of both modules. The results highlight CemoBAM's robustness and potential for advancing multimodal SER applications.
},
  ieeexplore={https://ieeexplore.ieee.org/document/11181320},
  address = {Kaohsiung, Taiwan},
  organization = {National Sun Yat-sen University},
  code = {https://github.com/nhut-ngnn/CemoBAM},
  selected = {true},
  preview = {CemoBAM.jpg}
}

@INPROCEEDINGS{phan2025ALMUS,
  abbr={APNOMS},
  bibtex_show=True,
  title = {ALMUS: Enhancing Active Learning for Object Detection with Metric-Based Uncertainty Sampling},
  author = {Phan, Duc Tai and Nguyen, Nhut Minh and Nguyen, Khang Phuc and Pham, Tri and Dang, Duc Ngoc Minh},
  booktitle = {2025 Asia-Pacific Network Operations and Management Symposium (APNOMS)},
  doi = {10.23919/apnoms67058.2025.11181447},
  year = {2025},
  month = {September},
  abstract ={Object detection is critical in computer vision but often requires large amounts of labeled data for effective training. Active learning (AL) has emerged as a promising solution to reduce the annotation burden by selecting the most informative samples for labeling. However, existing AL methods for object detection primarily focus on uncertainty sampling, which may not effectively balance the dual challenges of classification and localization. In this study, we explore active learning for object detection, with the objective of optimizing model performance while substantially reducing the demand for annotated data. We propose a novel Active Learning with Metric-based Uncertainty Sampling (ALMUS) that works effectively for the object detection task. This approach prioritizes selecting images containing objects from categories where the model exhibits suboptimal performance, as determined by category-specific evaluation metrics. To balance the annotation budget across different object classes, we propose a dynamic allocation strategy that considers the difficulty of each class and the distribution of object instances within the dataset. This combination of strategies enables our method to effectively address the dual challenges of classification and localization in object detection tasks while still focusing on the rarest and most challenging classes. We conduct extensive experiments on the PASCAL VOC 2007 and 2012 datasets, demonstrating that our method outperforms several active learning baselines. Our results indicate that the proposed approach enhances model performance and accelerates convergence, making it a valuable contribution to the field of active learning in object detection.
},
  ieeexplore={https://ieeexplore.ieee.org/document/11181447},
  address = {Kaohsiung, Taiwan},
  organization = {National Sun Yat-sen University},
  preview = {ALMUS.jpg}
}

@INPROCEEDINGS{hoang2025yolo,
  abbr={APNOMS},
  bibtex_show=True,
  title = {YOLO-Powered Traffic Sign Detection and OpenStreetMap Integration for Intelligent Navigation},
  author = {Hoang, Duc-Hieu and Nguyen, Dinh Thuan and Tran, Quoc Huy and Nguyen, Thanh Trung and Nguyen, Nhut Minh and Dang, Duc Ngoc Minh},
  doi = {10.23919/apnoms67058.2025.11181312},
  booktitle = {2025 Asia-Pacific Network Operations and Management Symposium (APNOMS)},
  year = {2025},
  month = {September},
  abstract = {This study presents the development of a deep learning-based traffic sign recognition system integrated with OpenStreetMap (OSM) to enhance smart navigation and traffic management. The system utilizes a You Only Look Once (YOLO)based model trained on the Vietnamese Camera-based Traffic Sign Recognition 47 (VCTSR47) dataset to detect and classify Vietnamese traffic signs accurately. A dashboard camera mounted on motorbikes or cars collects real-world traffic data, automatically recognizing traffic signs, extracting Global Positioning System (GPS) coordinates, and storing relevant information in a structured database. The extracted data is integrated into OSM via Application Programming Interfaces (APIs), enabling precise visualization and real-time alerts for critical traffic signs. The system’s performance is rigorously evaluated through various metrics, including mean Average Precision (mAP), Precision, Recall, and Frames Per Second (FPS). Experimental results confirm high recognition accuracy and real-time processing efficiency, demonstrating the feasibility and potential applications of deep learning and digital mapping in modern intelligent transportation systems.
},
  ieeexplore={https://ieeexplore.ieee.org/document/11181312},
  address = {Kaohsiung, Taiwan},
  organization = {National Sun Yat-sen University},
  preview = {YOLO.jpg}
}

@article{nguyen2025fleser,
  abbr={Preprint},
  bibtex_show={true},
  title={Fleser: Multimodal Emotion Recognition Via Dynamic Fuzzy Membership and Attention Fusion},
  author={Nguyen, Nhut Minh and Nguyen, Minh Trung and Nguyen, Thanh Trung and Tran, Phuong-Nam and Pham, Nhat Truong and Le, Linh and OTHMANI, Alice and Saddik, Abdulmotaleb El and Dang, Duc Ngoc Minh},
  journal={},
  year={2025},
  month={June},
  abstract={Multimodal learning has been demonstrated to improve classification outcomes in speech emotion recognition (SER). Despite this advantage, multimodal approaches in SER often face key challenges such as limited robustness in handling uncertainty, difficulties in generalizing across diverse emotional contexts, and inefficiencies in integrating heterogeneous modalities. To overcome these constraints, we propose FleSER, a multimodal emotion recognition framework that utilizes dynamic fuzzy membership and attention fusion. In this architecture, we introduce a rule-based dynamic fuzzy membership mechanism that adaptively transforms features. The FleSER architecture leverages audio and textual modalities, employing self-modality and cross-modality attention mechanisms with the α interpolation to capture complementary emotional cues. The α interpolation-based feature fusion mechanism adaptively emphasizes the more informative modality in varying contexts, ensuring robust multimodal integration. This comprehensive design improves the model's recognition accuracy. We evaluate the FleSER architecture on the three benchmark datasets: IEMOCAP, ESD, and MELD. FleSER surpasses the previous state-of-the-art (SOTA) by 1.92% on IEMOCAP and an impressive 6.37% on ESD, demonstrating its superior effectiveness in enhancing emotion recognition accuracy across various datasets. Ablation studies further validate the effectiveness of each key component, including unimodal and multimodal input effectiveness, fuzzy membership functions, fusion strategies, and projection dimension on FleSER architecture performance. Our source code is publicly available at https://github.com/nhut-ngnn/FleSER.},
  ssrn={5316634},
  code={https://github.com/aita-lab/FleSER},
  selected={false},
  preprint={true},
  google_scholar_id={2osOgNQ5qMEC},
  preview={FleSER.png}
}

@article{nguyen2025MSER,
  abbr={Preprint},
  bibtex_show={true},
  title={Multi-modal fusion in speech emotion recognition: A comprehensive review of methods and technologies},
  author={Nguyen, Nhut Minh and Nguyen, Thanh Trung and Tran, Phuong-Nam and Lim, Chee Peng and Pham, Nhat Truong and Dang, Duc Ngoc Minh},
  journal={},
  year={2024},
  abstract={Speech emotion recognition (SER) plays a crucial role in human-computer interaction, enhancing numerous applications such as virtual assistants, healthcare monitoring, and customer support by identifying and interpreting emotions conveyed through spoken language. While single-modality SER systems demonstrate notable simplicity and computational efficiency, excelling in extracting critical features like vocal prosody and linguistic content, there is a pressing need to improve their performance in challenging conditions, such as noisy environments and the handling of ambiguous expressions or incomplete information. These challenges underscore the necessity of transitioning to multi-modal approaches, which integrate complementary data sources to achieve more robust and accurate emotion detection. With advancements in artificial intelligence, especially in neural networks and deep learning, many studies have employed advanced deep learning and feature fusion techniques to enhance SER performance. This review synthesizes comprehensive publications from 2020 to 2024, exploring prominent multi-modal fusion strategies, including early fusion, late fusion, deep fusion, and hybrid fusion methods, while also examining data representation, data translation, attention mechanisms, and graph-based fusion technologies. We assess the effectiveness of various fusion techniques across standard SER datasets, highlighting their performance in diverse tasks and addressing challenges related to data alignment, noise management, and computational demands. Additionally, we explore potential future directions for enhancing multi-modal SER systems, emphasizing scalability and adaptability in real-world applications. This survey aims to contribute to the advancement of multi-modal SER and to inform researchers about effective fusion strategies for developing more responsive and emotion-aware systems.},
  ssrn={5063214},
  code={https://github.com/aita-lab/awesome-multimodal-fusion-emotion-recognition/},
  selected={true},
  preprint={true},
  google_scholar_id={9yKSN-GCB0IC},
  preview={MMF_SER_Review.jpg}
}

@INPROCEEDINGS{nguyen2024age-gender,
  abbr={ICTC},
  bibtex_show=True,
  title={Voice-Based Age and Gender Recognition: A Comparative Study of LSTM, RezoNet and Hybrid CNNs-BiLSTM Architecture},
  author={Nguyen, Nhut Minh and Nguyen, Thanh Trung and Nguyen, Hua Hiep and Tran, Phuong-Nam and Dang, Duc Ngoc Minh},
  booktitle={2024 15th International Conference on Information and Communication Technology Convergence (ICTC)},
  pages={1--1},
  year={2024},
  publisher={IEEE},
  abstract={In this study, we compared three architectures for the task of age and gender recognition from voice data: Long Short-Term Memory networks (LSTM), Hybrid of Convolutional Neural Networks and Bidirectional Long Short-Term Memory (CNNs-BiLSTM), and the recently released RezoNet architecture. The dataset used in this study was sourced from Mozilla Common Voice in Japanese. Features such as pitch, magnitude, Mel-frequency cepstral coefficients (MFCCs), and filter-bank energies were extracted from the voice data for signal processing, and the three architectures were evaluated. Our evaluation revealed that LSTM was slightly less accurate than RezoNet (83.1%), with the hybrid CNNs-BiLSTM (93.1%) and LSTM achieving the highest accuracy for gender recognition (93.5%). However, hybrid CNNs-BiLSTM architecture outperformed the other models in age recognition, achieving an accuracy of 69.75%, compared to 64.25% and 44.88% for LSTM and RezoNet, respectively. Using Japanese language data and the extracted characteristics, the hybrid CNNs-BiLSTM architecture model demonstrated the highest accuracy in both tests, highlighting its efficacy in voice-based age and gender detection. These results suggest promising avenues for future research and practical applications in this field.},
  ieeexplore={https://ieeexplore.ieee.org/document/10827387},
  doi={10.1109/ICTC62082.2024.10827387},
  code={https://github.com/nhut-ngnn/Voice-Based-Age-and-Gender-Recogniton},
  selected={true},
  dimensions={true},
  google_scholar_id={d1gkVwhDpl0C},
  preview={age-gender.png}
}
